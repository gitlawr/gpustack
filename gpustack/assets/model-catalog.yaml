draft_models:
- name: Qwen3-8B-EAGLE3
  algorithm: eagle3
  source: huggingface
  huggingface_repo_id: Tengyunw/qwen3_8b_eagle3
- name: Qwen3-30B-A3B-EAGLE3
  algorithm: eagle3
  source: huggingface
  huggingface_repo_id: Tengyunw/qwen3_30b_moe_eagle3
- name: Qwen3-235B-A22B-EAGLE3
  algorithm: eagle3
  source: huggingface
  huggingface_repo_id: lmsys/Qwen3-235B-A22B-EAGLE3
- name: gpt-oss-120b-EAGLE3
  algorithm: eagle3
  source: huggingface
  huggingface_repo_id: lmsys/EAGLE3-gpt-oss-120b-bf16
model_sets:
- name: Qwen3 30B A3B
  description: A MoE model with 30B parameters and 3B activated parameters in Qwen3 series.
  home: https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507
  icon: /static/catalog_icons/qwen.png
  size: 30
  activated_size: 3 
  categories:
    - llm
  capabilities:
    - context/1M
    - tools
  licenses:
    - apache-2.0
  release_date: "2025-07-21"
  recipes:
    - mode: throughput
      quantization: FP8
      source: huggingface
      huggingface_repo_id: Qwen/Qwen3-30B-A3B-FP8
      gpu_filters:
      - gpu_type: H100
        gpu_count: 1
      backend: SGLang
      backend_parameters:
        - --reasoning-parser=qwen3
        - --tool-call-parser=qwen25
    - mode: latency
      quantization: FP8
      source: huggingface
      huggingface_repo_id: Qwen/Qwen3-30B-A3B-FP8
      gpu_filters:
      - gpu_type: H100
        gpu_count: 1
      speculative_config:
        enabled: true
        algorithm: eagle3
        draft_model_name: Qwen3-30B-A3B-EAGLE3
        num_draft_tokens: 8
      backend: SGLang
      backend_parameters:
        - --reasoning-parser=qwen3
        - --tool-call-parser=qwen25
        - --speculative-algorithm=EAGLE3
        - --speculative-draft-model-path=Tengyunw/qwen3_30b_moe_eagle3
        - --speculative-num-steps=6
        - --speculative-eagle-topk=10
        - --speculative-num-draft-tokens=32
    - mode: reference
      quantization: BF16
      source: huggingface
      huggingface_repo_id: Qwen/Qwen3-30B-A3B
      backend: vLLM
      backend_parameters:
        - --tool-call-parser=hermes
        - --enable-auto-tool-choice
        - --max-model-len=32768
- name: GLM-4.6
  description: GLM-4.6 is a large language model developed by Zhipu AI, featuring advanced agentic, reasoning, and coding capabilities.
  home: https://huggingface.co/zai-org/GLM-4.6
  icon: /static/catalog_icons/zai.png
  size: 355
  activated_size: 32
  categories:
    - llm
  capabilities:
    - context/1M
    - reasoning
    - tools
  licenses:
    - mit
  release_date: "2025-09-30"
  recipes:
    - mode: throughput
      quantization: FP8
      source: huggingface
      huggingface_repo_id: zai-org/GLM-4.6-FP8
      gpu_filters:
      - gpu_type: "H100"
        gpu_count: 8
      backend: SGLang
      backend_parameters:
        - --reasoning-parser=glm45
        - --tool-call-parser=glm45
        - --tp-size=8
        - --ep-size=8
    - mode: throughput
      quantization: FP8
      source: huggingface
      huggingface_repo_id: zai-org/GLM-4.6-FP8
      gpu_filters:
      - gpu_type: "A100"
        gpu_count: 8
      backend: SGLang
      backend_parameters:
        - --reasoning-parser=glm45
        - --tool-call-parser=glm45
        - --enable-auto-tool-choice
        - --max-num-batched-tokens=8192
        - --tp-size=8
    - mode: latency
      quantization: FP8
      source: huggingface
      huggingface_repo_id: zai-org/GLM-4.5
      speculative_config:
        enabled: true
        algorithm: mtp
        num_draft_tokens: 4
      backend: SGLang
      backend_parameters:
        - --reasoning-parser=glm45
        - --tool-call-parser=glm45
    - mode: reference
      quantization: BF16
      source: huggingface
      huggingface_repo_id: zai-org/GLM-4.6
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=glm45
        - --tool-call-parser=glm45
        - --enable-auto-tool-choice
        - --max-model-len=32768
- name: gpt-oss 120B
  description: The gpt-oss series is OpenAI's family of open-weight models, designed for powerful reasoning, agentic tasks, and versatile developer use cases.
  home: https://openai.com
  icon: /static/catalog_icons/openai.png
  categories:
    - llm
  capabilities:
    - context/128K
  size: 120
  licenses:
    - apache-2.0
  release_date: "2025-08-05"
  recipes:
    - mode: throughput
      quantization: "MXFP4"
      source: huggingface
      huggingface_repo_id: openai/gpt-oss-120b
      gpu_filters:
      - gpu_type: "H100"
        gpu_count: 2
      backend: vLLM
      backend_parameters:
        - --max-model-len=32768
        - --tool-call-parser=openai
        - --enable-auto-tool-choice
        - --async-scheduling
        - --tensor-parallel-size=2
    - mode: throughput
      quantization: "MXFP4"
      source: huggingface
      huggingface_repo_id: openai/gpt-oss-120b
      backend: vLLM
      backend_parameters:
        - --max-model-len=32768
        - --tool-call-parser=openai
        - --enable-auto-tool-choice
        - --async-scheduling
    - mode: reference
      quantization: "MXFP4"
      source: huggingface
      huggingface_repo_id: openai/gpt-oss-120b
      backend: vllm
      backend_parameters:
        - --max-model-len=32768
        - --tool-call-parser=openai
        - --enable-auto-tool-choice
- name: gpt-oss 20B
  description: The gpt-oss series is OpenAI's family of open-weight models, designed for powerful reasoning, agentic tasks, and versatile developer use cases.
  home: https://openai.com
  icon: /static/catalog_icons/openai.png
  categories:
    - llm
  capabilities:
    - context/128K
  size: 20
  licenses:
    - apache-2.0
  release_date: "2025-08-05"
  recipes:
    - mode: reference
      quantization: "MXFP4"
      source: huggingface
      huggingface_repo_id: openai/gpt-oss-20b
      backend: vllm
      backend_parameters:
        - --max-model-len=32768
        - --tool-call-parser=openai
        - --enable-auto-tool-choice
- name: Deepseek R1 0528
  description: DeepSeek-R1-0528 is a minor version of the DeepSeek R1 model that features enhanced reasoning depth and inference capabilities. These improvements are achieved through increased computational resources and algorithmic optimizations applied during post-training. The model delivers strong performance across a range of benchmark evaluations, including mathematics, programming, and general logic, with overall capabilities approaching those of leading models such as O3 and Gemini 2.5 Pro.
  home: https://www.deepseek.com
  icon: /static/catalog_icons/deepseek.png
  categories:
    - llm
  capabilities:
    - context/128K
  sizes:
    - 671
  licenses:
    - mit
  release_date: "2025-05-28"
  recipes:
    - mode: throughput
      quantization: FP8
      source: huggingface
      huggingface_repo_id: deepseek-ai/DeepSeek-R1-0528
      gpu_filters:
      - gpu_type: H200
        gpu_count: 8
      backend: SGLang
      backend_parameters:
        - --tp-size=8
        - --enable-dp-attention
    - mode: reference
      quantization: FP8
      source: huggingface
      huggingface_repo_id: deepseek-ai/DeepSeek-R1-0528
      backend: vLLM
      backend_parameters:
        - --max-model-len=32768
# Embedding models
- name: Qwen3 Embedding 0.6B
  description: Qwen3-Embedding is a multilingual embedding model series optimized for retrieval, clustering, classification, and bitext mining. It supports 100+ languages, with flexible vector dimensions and instruction tuning.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 0.6
  categories:
    - embedding
  capabilities:
    - dimensions/4096
    - max_tokens/32K
  licenses:
    - apache-2.0
  release_date: "2025-06-09"
  recipes:
    - mode: reference
      quantization: "BF16"
      source: huggingface
      huggingface_repo_id: Qwen/Qwen3-Embedding-0.6B
      categories:
        - embedding
      backend: vLLM
      backend_parameters:
        - --task=embed
- name: Qwen3 Embedding 4B
  description: Qwen3-Embedding is a multilingual embedding model series optimized for retrieval, clustering, classification, and bitext mining. It supports 100+ languages, with flexible vector dimensions and instruction tuning.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 4
  categories:
    - embedding
  capabilities:
    - dimensions/4096
    - max_tokens/32K
  licenses:
    - apache-2.0
  release_date: "2025-06-09"
  recipes:
    - mode: reference
      quantization: "BF16"
      source: huggingface
      huggingface_repo_id: Qwen/Qwen3-Embedding-4B
      categories:
        - embedding
      backend: vLLM
      backend_parameters:
        - --task=embed
- name: Qwen3 Embedding 8B
  description: Qwen3-Embedding is a multilingual embedding model series optimized for retrieval, clustering, classification, and bitext mining. It supports 100+ languages, with flexible vector dimensions and instruction tuning.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 8
  categories:
    - embedding
  capabilities:
    - dimensions/4096
    - max_tokens/32K
  licenses:
    - apache-2.0
  release_date: "2025-06-09"
  recipes:
    - mode: reference
      quantization: "BF16"
      source: huggingface
      huggingface_repo_id: Qwen/Qwen3-Embedding-8B
      categories:
        - embedding
      backend: vLLM
      backend_parameters:
        - --task=embed
# Reranker models
- name: Qwen3 Reranker 0.6B
  description: Qwen3-Reranker is a multilingual text reranking model series optimized for retrieval, clustering, classification, and bitext mining. It supports 100+ languages, with flexible vector dimensions and instruction tuning.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 0.6
  categories:
    - reranker
  capabilities:
    - max_tokens/32K
  licenses:
    - apache-2.0
  release_date: "2025-06-09"
  recipes:
    - mode: reference
      quantization: "BF16"
      source: huggingface
      huggingface_repo_id: Qwen/Qwen3-Reranker-0.6B
      categories:
        - reranker
      env:
        GPUSTACK_APPLY_QWEN3_RERANKER_TEMPLATES: "true"
      backend: vLLM
      backend_parameters:
        - '--hf_overrides={"architectures": ["Qwen3ForSequenceClassification"],"classifier_from_token": ["no", "yes"],"is_original_qwen3_reranker": true}'
        - --task=score
- name: Qwen3 Reranker 4B
  description: Qwen3-Reranker is a multilingual text reranking model series optimized for retrieval, clustering, classification, and bitext mining. It supports 100+ languages, with flexible vector dimensions and instruction tuning.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 4
  categories:
    - reranker
  capabilities:
    - max_tokens/32K
  licenses:
    - apache-2.0
  release_date: "2025-06-09"
  recipes:
    - mode: reference
      quantization: "BF16"
      source: huggingface
      huggingface_repo_id: Qwen/Qwen3-Reranker-4B
      categories:
        - reranker
      env:
        GPUSTACK_APPLY_QWEN3_RERANKER_TEMPLATES: "true"
      backend: vLLM
      backend_parameters:
        - '--hf_overrides={"architectures": ["Qwen3ForSequenceClassification"],"classifier_from_token": ["no", "yes"],"is_original_qwen3_reranker": true}'
        - --task=score
- name: Qwen3 Reranker 8B
  description: Qwen3-Reranker is a multilingual text reranking model series optimized for retrieval, clustering, classification, and bitext mining. It supports 100+ languages, with flexible vector dimensions and instruction tuning.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 8
  categories:
    - reranker
  capabilities:
    - max_tokens/32K
  licenses:
    - apache-2.0
  release_date: "2025-06-09"
  recipes:
    - mode: reference
      quantization: "BF16"
      source: huggingface
      huggingface_repo_id: Qwen/Qwen3-Reranker-8B
      categories:
        - reranker
      env:
        GPUSTACK_APPLY_QWEN3_RERANKER_TEMPLATES: "true"
      backend: vLLM
      backend_parameters:
        - '--hf_overrides={"architectures": ["Qwen3ForSequenceClassification"],"classifier_from_token": ["no", "yes"],"is_original_qwen3_reranker": true}'
        - --task=score
